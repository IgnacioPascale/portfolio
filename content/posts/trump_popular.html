---
title: "How popular is Donald Trump?"
date: '2020-09-20'
description: Analysis on his first presidential cycle
draft: no
keywords: ''
slug: trump_popular
categories: null
---

<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<p>Mr Donald Trump is undoubtely one of the most controversial persons in the world. Some people love him, others not so much. And there’s a lot going on about his approval index.</p>
<p><strong>But how popular is, in reality, Mr. President?</strong></p>
<p>fivethirtyeight.com has detailed data on <a href="https://projects.fivethirtyeight.com/trump-approval-ratings">all polls that track the president’s approval</a>. You can find this information on the link.</p>
<p>In this blogpost, we will download the data set <code>approval_pollist</code> to analyse Mr. Trump’s popularity throughout his presidential cycle. The idea is to produce a plot that will let us analyse this variable each year, and introduce an interval confidence for every point.</p>
<div id="exploratory-data-analysis" class="section level2">
<h2>Exploratory Data Analysis</h2>
<p>Let’s import our data and have a first view using <code>skim()</code>.</p>
<pre class="r"><code># Import approval polls data
approval_polllist &lt;- read_csv(here::here(&#39;datasets&#39;, &#39;approval_polllist.csv&#39;))

skim(approval_polllist)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-1">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">approval_polllist</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">14533</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">22</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">12</td>
</tr>
<tr class="odd">
<td align="left">logical</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">9</td>
</tr>
<tr class="odd">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">president</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">12</td>
<td align="right">12</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">subgroup</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">6</td>
<td align="right">9</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">modeldate</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">9</td>
<td align="right">9</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">startdate</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">8</td>
<td align="right">10</td>
<td align="right">0</td>
<td align="right">1316</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">enddate</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">8</td>
<td align="right">10</td>
<td align="right">0</td>
<td align="right">1310</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">pollster</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">4</td>
<td align="right">52</td>
<td align="right">0</td>
<td align="right">86</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">grade</td>
<td align="right">485</td>
<td align="right">0.97</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">14</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">population</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">4</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">multiversions</td>
<td align="right">14463</td>
<td align="right">0.00</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">url</td>
<td align="right">4</td>
<td align="right">1.00</td>
<td align="right">21</td>
<td align="right">283</td>
<td align="right">0</td>
<td align="right">3278</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">createddate</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">8</td>
<td align="right">10</td>
<td align="right">0</td>
<td align="right">1161</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">timestamp</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">20</td>
<td align="right">20</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: logical</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="left">count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">tracking</td>
<td align="right">8713</td>
<td align="right">0.4</td>
<td align="right">1</td>
<td align="left">TRU: 5820</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table style="width:100%;">
<colgroup>
<col width="17%" />
<col width="8%" />
<col width="12%" />
<col width="8%" />
<col width="8%" />
<col width="7%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="5%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">samplesize</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1880.04</td>
<td align="right">2723.11</td>
<td align="right">121.0</td>
<td align="right">1000.00</td>
<td align="right">1500.00</td>
<td align="right">1.87e+03</td>
<td align="right">5.54e+04</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">weight</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.34</td>
<td align="right">0.48</td>
<td align="right">0.0</td>
<td align="right">0.12</td>
<td align="right">0.13</td>
<td align="right">2.20e-01</td>
<td align="right">3.85e+00</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">influence</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.00</td>
<td align="right">0.03</td>
<td align="right">0.0</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00e+00</td>
<td align="right">9.40e-01</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">approve</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">42.29</td>
<td align="right">3.60</td>
<td align="right">23.9</td>
<td align="right">40.00</td>
<td align="right">42.00</td>
<td align="right">4.50e+01</td>
<td align="right">5.90e+01</td>
<td align="left">▁▁▇▃▁</td>
</tr>
<tr class="odd">
<td align="left">disapprove</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">53.35</td>
<td align="right">3.13</td>
<td align="right">35.0</td>
<td align="right">51.00</td>
<td align="right">53.00</td>
<td align="right">5.50e+01</td>
<td align="right">7.59e+01</td>
<td align="left">▁▃▇▁▁</td>
</tr>
<tr class="even">
<td align="left">adjusted_approve</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">41.53</td>
<td align="right">2.75</td>
<td align="right">25.1</td>
<td align="right">39.79</td>
<td align="right">41.67</td>
<td align="right">4.34e+01</td>
<td align="right">5.48e+01</td>
<td align="left">▁▁▇▃▁</td>
</tr>
<tr class="odd">
<td align="left">adjusted_disapprove</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">53.35</td>
<td align="right">2.68</td>
<td align="right">36.8</td>
<td align="right">51.80</td>
<td align="right">53.36</td>
<td align="right">5.49e+01</td>
<td align="right">7.01e+01</td>
<td align="left">▁▁▇▁▁</td>
</tr>
<tr class="even">
<td align="left">poll_id</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">57808.04</td>
<td align="right">6301.87</td>
<td align="right">49232.0</td>
<td align="right">51007.00</td>
<td align="right">57471.00</td>
<td align="right">6.34e+04</td>
<td align="right">6.82e+04</td>
<td align="left">▇▂▇▂▆</td>
</tr>
<tr class="odd">
<td align="left">question_id</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">98218.15</td>
<td align="right">17655.69</td>
<td align="right">77244.0</td>
<td align="right">80762.00</td>
<td align="right">93064.00</td>
<td align="right">1.16e+05</td>
<td align="right">1.28e+05</td>
<td align="left">▇▅▂▂▆</td>
</tr>
</tbody>
</table>
<p>As it seems, our dataset contains a total of 14533 observations and 22 variables, from which 12 are <code>character</code> variables, 1 <code>logical</code> and 9 <code>numeric</code>.</p>
<p>This one also seems to be a clean data set; there are no <code>NA</code> values in our <code>numeric</code> variables, according to our <code>skim</code>. Unfortunately, our <code>logical</code> variable <code>tracking</code> is missing 8713 records, and our <code>character</code> variables <code>multiversions</code> and <code>grade</code> are missing 14463 and 485 records accordingly.</p>
<p>Let’s now take a raw view at our data using the function <code>sample_n</code>.</p>
<pre class="r"><code># Take a sample size
sample_n(approval_polllist, 5)</code></pre>
<pre><code>## # A tibble: 5 x 22
##   president subgroup modeldate startdate enddate pollster grade samplesize
##   &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;
## 1 Donald T… All pol… 8/29/2020 1/16/2020 1/20/2… Monmout… A+           903
## 2 Donald T… All pol… 8/29/2020 9/17/2017 9/21/2… Ipsos    B-          1625
## 3 Donald T… All pol… 8/29/2020 9/16/2019 9/17/2… Ipsos    B-          1116
## 4 Donald T… All pol… 8/29/2020 5/9/2017  5/13/2… Ipsos    B-          1840
## 5 Donald T… Adults   8/29/2020 5/24/2019 5/27/2… Morning… B/C         2201
## # … with 14 more variables: population &lt;chr&gt;, weight &lt;dbl&gt;, influence &lt;dbl&gt;,
## #   approve &lt;dbl&gt;, disapprove &lt;dbl&gt;, adjusted_approve &lt;dbl&gt;,
## #   adjusted_disapprove &lt;dbl&gt;, multiversions &lt;chr&gt;, tracking &lt;lgl&gt;, url &lt;chr&gt;,
## #   poll_id &lt;dbl&gt;, question_id &lt;dbl&gt;, createddate &lt;chr&gt;, timestamp &lt;chr&gt;</code></pre>
<p>The good news is that in our analysis we will not need any of the 3 variables with missing values. Therefore, we can dismiss them and continue with our analysis.</p>
</div>
<div id="data-wrangling" class="section level2">
<h2>Data wrangling</h2>
<p>Our variables of interest are:</p>
<ol style="list-style-type: decimal">
<li><code>approve</code>: approval rate</li>
<li><code>disapprove</code>: disapproval rate</li>
<li><code>poll_id</code>: poll_id</li>
<li><code>enddate</code>: to associate the “grade” or approval to a date</li>
</ol>
<p>First, we filter the data on the <code>subgroup</code> “Voters”, as this is what we are interested in, and then <code>select()</code> our variables of interest.</p>
<pre class="r"><code># Create new df object

df &lt;- approval_polllist %&gt;% 
  filter(subgroup %in% &quot;Voters&quot;) %&gt;% 
  select(enddate, approve, disapprove, poll_id) #include poll_id to check for duplicate polls

# Check for duplicates and missing entries
any(duplicated(df))</code></pre>
<pre><code>## [1] FALSE</code></pre>
<pre class="r"><code>any(is.na(df))</code></pre>
<pre><code>## [1] FALSE</code></pre>
<pre class="r"><code>glimpse(df)</code></pre>
<pre><code>## Rows: 4,783
## Columns: 4
## $ enddate    &lt;chr&gt; &quot;1/22/2017&quot;, &quot;1/24/2017&quot;, &quot;1/25/2017&quot;, &quot;1/25/2017&quot;, &quot;1/24/…
## $ approve    &lt;dbl&gt; 46.0, 45.2, 42.6, 36.0, 57.0, 43.0, 44.0, 59.0, 41.3, 45.1…
## $ disapprove &lt;dbl&gt; 37.0, 44.3, 47.8, 44.0, 43.0, 39.0, 44.0, 41.0, 48.5, 46.9…
## $ poll_id    &lt;dbl&gt; 49249, 49426, 49425, 49260, 49266, 49254, 49237, 49247, 49…</code></pre>
<p>There are neither duplicates nor missing values - as confirmed before.</p>
<p>We can now wrangle the data to sort it by date, group it per week and year. For this purpose, we will use <code>mutate()</code> and the functions in the library <code>lubridate</code> that will help us manage dates. Also, we will create confindence intervals for the sample mean of the <code>approve</code>-<code>disapprove</code> - this is the population parameter we are interested in.</p>
<pre class="r"><code>grouped_df &lt;- df %&gt;% 
  #get year and week number
  mutate(date = mdy(enddate), 
         weeknr = week(date), 
         yearnr = year(date)) %&gt;%
  
  #group poll date per week
  group_by(yearnr, weeknr) %&gt;%
  
  #summarise statistics of the grouped data
  summarise(avg_net_apr = mean(approve - disapprove), 
            std = sd(approve - disapprove), 
            count = n(), 
            t_crit = qt(.975, count-1)) %&gt;% 
  mutate(se = std / sqrt(count),
  
  #calculate confidence interval
        upper = avg_net_apr + t_crit*se,
        lower = avg_net_apr - t_crit*se) 
  
grouped_df</code></pre>
<pre><code>## # A tibble: 191 x 9
## # Groups:   yearnr [4]
##    yearnr weeknr avg_net_apr   std count t_crit    se   upper   lower
##     &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1   2017      4       3.26   8.11    13   2.18 2.25   8.17    -1.64 
##  2   2017      5       2.21   3.46    18   2.11 0.815  3.93     0.491
##  3   2017      6      -0.933  5.36    24   2.07 1.09   1.33    -3.20 
##  4   2017      7      -2.64   6.28    25   2.06 1.26  -0.0462  -5.23 
##  5   2017      8      -3.83   6.22    24   2.07 1.27  -1.21    -6.46 
##  6   2017      9      -0.955  4.21    22   2.08 0.897  0.912   -2.82 
##  7   2017     10      -2.06   4.47    26   2.06 0.877 -0.254   -3.87 
##  8   2017     11      -5.53   3.77    25   2.06 0.754 -3.97    -7.08 
##  9   2017     12      -4.38   6.78    25   2.06 1.36  -1.58    -7.18 
## 10   2017     13      -9.71   3.56    25   2.06 0.712 -8.24   -11.2  
## # … with 181 more rows</code></pre>
<p>Our data frame is now grouped by year, week and includes our <code>avg_net_apr</code> (sample mean), <code>std</code>, <code>se</code> and both <code>upper</code> and <code>lower</code> confidence intervals. We are ready to plot this.</p>
</div>
<div id="plotting" class="section level2">
<h2>Plotting</h2>
<p>Now, we can construct the plot we want to analyze.</p>
<pre class="r"><code>grouped_df %&gt;%
  # Create ggplot
  ggplot(aes(weeknr, avg_net_apr))+
  
  #create facet wrap 
  facet_wrap(~yearnr)+
  
  #add line with dots; colour according to the year
  geom_path(aes(color = factor(yearnr))) + geom_point(aes(color = factor(yearnr)), size = 0.85) +
  
  #add band around line to display CI
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = factor(yearnr), color = factor(yearnr)),
              alpha = 0.1)+ #color adds border on the edge
  
  #horizontal line on y=0
  geom_hline(yintercept=0, color = &quot;orange&quot;)+
  
  #adjust scales
  scale_x_continuous(expand = expansion(mult = .07), #white border around line plots
                     breaks=seq(0,52,13)) + #x-axis breaks
  scale_y_continuous(breaks = breaks_extended(n=12)) + #12 breaks on y-axis
  
  #aesthetics
  theme_bw() + #black border around plots
  theme(aspect.ratio = 1/3, legend.position = &quot;none&quot;) + 
  labs(x = &quot;Week of the year&quot;, 
       y = &quot;Averge Net Approval (%)&quot;, 
       title = &quot;Trump&#39;s popularity remained steadily below 50% throughout his term&quot;, 
       subtitle = &quot;Estimated Net Approval (approve-disapprove) for Donald Trump&quot;)</code></pre>
<p><img src="/posts/trump_popular_files/figure-html/plot-1.png" width="960" style="display: block; margin: auto;" /></p>
<p><strong>What is this telling us?</strong></p>
<p>The graph shows that Trump’s approval rate has decreased very rapidly below the 50% mark in the early course of his presidency, and has since then remained fairly stable at roughly 40%.</p>
<p>The early weeks of 2020 show a further decrease in popularity. This coincides with the start of the corona crisis, and subsequent restricting measures imposed on the population. As these were sometimes seen as a restriction of <em>freedom</em>, they were not well tolerated by some.</p>
<p>In later weeks of 2020, Trump’s populariy has seen a new rise, as restrictions were lifted and the presidential election campaigns have started going at full steam.</p>
</div>
<div id="comparing-confidence-intervals" class="section level2">
<h2>Comparing Confidence Intervals</h2>
<p>Taking a look at year 2020, there’s a big difference in some confidence intervals. In the first part of the year, these were narrow - however, more variability is present in the last bits. Let’s analyse the the confidence intervals for <code>week 15</code> (6-12 April 2020) and <code>week 34</code> (17-23 August 2020) and see what they are telling us.</p>
<p><strong>What’s going on?</strong></p>
<pre class="r"><code>grouped_df %&gt;% 
  filter(weeknr == 15, yearnr == 2020) %&gt;% 
  select(lower, upper)</code></pre>
<pre><code>## # A tibble: 1 x 3
## # Groups:   yearnr [1]
##   yearnr lower upper
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1   2020 -8.92 -6.78</code></pre>
<pre class="r"><code>grouped_df %&gt;% 
  filter(weeknr == 34, yearnr == 2020) %&gt;% 
  select(lower, upper)</code></pre>
<pre><code>## # A tibble: 1 x 3
## # Groups:   yearnr [1]
##   yearnr lower upper
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1   2020 -12.3 -5.83</code></pre>
<p>It is evident that the confidence interval for week 34 is larger than for week 15. Two elements influence this difference. The first one is the difference in standard deviation: the SD for week 34 is far higher than for week 15. This means that there was more uncertainty among the polls as to the approval rate, which might be due to biased polling techniques or a general uncertainty in the public in the light of the upcoming elections. The second factor is that there are more polls available for week 15 (28 as opposed to 25 for week 34), and a higher sample size decreases the uncertainty.</p>
</div>
